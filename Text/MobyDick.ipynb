{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load necessary libraries \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('mobydick.txt', origin='http://www.gutenberg.org/files/2701/2701-0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, encoding='utf8') as f:\n",
    "    storyStart = f.readlines()[340:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = [word for storyStart in storyStart for word in storyStart.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ETYMOLOGY.',\n",
       " '(Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordList[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ETYMOLOGY.',\n",
       " 'Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordList = list(map(lambda each:each.strip(\"(\"), wordList))\n",
    "wordList = list(map(lambda each:each.strip(\")\"), wordList))\n",
    "wordList[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSet = sorted(set(wordList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nWordSet = len(wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nWordList = len(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moby Dick contains 214986 words/numbers/characters.\n",
      "\n",
      "Moby Dick contains 33415 unique words/numbers/characters\n"
     ]
    }
   ],
   "source": [
    "encoding = {j: i for i, j in enumerate(wordSet)}\n",
    "decoding = {i: j for i, j in enumerate(wordSet)}\n",
    "print()\n",
    "print(\"Moby Dick contains {0} words/numbers/characters.\".format(nWordList))\n",
    "print()\n",
    "print(\"Moby Dick contains {0} unique words/numbers/characters\".format(nWordSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = [], []\n",
    "sentenceLength = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10748 sentences of length 30 from our Moby Dick story\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, nWordList - sentenceLength, 20):\n",
    "    sentence = wordList[i:i+sentenceLength]\n",
    "    next_ = wordList[i + sentenceLength]\n",
    "    X_data.append([encoding[j] for j in sentence])\n",
    "    y_data.append([encoding[next_]])\n",
    "    \n",
    "print(\"We have {0} sentences of length {1} from our Moby Dick story\".format(len(X_data), sentenceLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization complete\n"
     ]
    }
   ],
   "source": [
    "# Vectorize data\n",
    "X = np.zeros((len(X_data), sentenceLength, nWordSet), dtype=np.bool)\n",
    "y = np.zeros((len(X_data), nWordSet), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1\n",
    "    \n",
    "print(\"Vectorization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(sentenceLength, nWordSet)))\n",
    "model.add(\n",
    "model.add(LSTM())\n",
    "model.add(Dense(nWordSet))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netArchitecture = model.to_yaml()\n",
    "with open('model.yaml', 'a') as model_file:\n",
    "    model_file.write(netArchitecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check before running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Double check y. Dimension: {0} # Sentences: {1} Words in Moby Dick: {2}\".format(y.shape, len(X_data), nWordSet))\n",
    "print(\"Double check X. Dimension: {0} Sentence length: {1}\".format(X.shape, sentenceLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new writings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
