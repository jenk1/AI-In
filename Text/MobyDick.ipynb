{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('mobydick.txt', origin='http://www.gutenberg.org/files/2701/2701-0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, encoding='utf8') as f:\n",
    "    story_start = f.readlines()[340:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [word for story_start in story_start for word in story_start.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ETYMOLOGY.',\n",
       " '(Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ETYMOLOGY.',\n",
       " 'Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list(map(lambda each:each.strip(\"(\"), word_list))\n",
    "word_list = list(map(lambda each:each.strip(\")\"), word_list))\n",
    "word_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = sorted(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33415"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_word_set = len(word_set)\n",
    "num_word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214986"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_word_list = len(word_list)\n",
    "num_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moby Dick contains 214986 words/symbols.\n",
      "\n",
      "Moby Dick contains 33415 unique words/symbols.\n"
     ]
    }
   ],
   "source": [
    "encoding = {j: i for i, j in enumerate(word_set)}\n",
    "decoding = {i: j for i, j in enumerate(word_set)}\n",
    "print()\n",
    "print(\"Moby Dick contains {0} words/symbols.\".format(num_word_list))\n",
    "print()\n",
    "print(\"Moby Dick contains {0} unique words/symbols.\".format(num_word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = [], []\n",
    "sentence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10748 sentences of length 30 from our Moby Dick story\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_word_list - sentence_length, 20):\n",
    "    sentence = word_list[i:i+sentence_length]\n",
    "    next_ = word_list[i + sentence_length]\n",
    "    X_data.append([encoding[j] for j in sentence])\n",
    "    y_data.append([encoding[next_]])\n",
    "    \n",
    "print(\"We have {0} sentences of length {1} from our Moby Dick story\".format(len(X_data), sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization complete\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(X_data), sentence_length, num_word_set), dtype=np.bool)\n",
    "y = np.zeros((len(X_data), num_word_set), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1\n",
    "    \n",
    "print(\"Vectorization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for X is (10748, 30, 33415) with each sentence being 30 words/symbols long\n",
      "Dimensions for y is (10748, 33415) with 10748 setences made from 33415 unique words/symbols\n"
     ]
    }
   ],
   "source": [
    "# Review dimensions for train and testing sets \n",
    "print(\"Dimensions for X is {0} with each sentence being {1} words/symbols long\".format(X.shape, sentence_length))\n",
    "print(\"Dimensions for y is {0} with {1} setences made from {2} unique words/symbols\".format(y.shape, len(X_data), num_word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 30, 128)           17174528  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 33415)             2171975   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 33415)             0         \n",
      "=================================================================\n",
      "Total params: 19,395,911\n",
      "Trainable params: 19,395,911\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(sentence_length, num_word_set), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(num_word_set))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_model = model.to_yaml()\n",
    "with open('model.yaml', 'a') as model_file:\n",
    "    model_file.write(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"weights-{epoch:02d}-{loss:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, nb_epoch=10, batch_size=32, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
